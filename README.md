# ğŸ”¬ Multi-Agent Research Intelligence Platform

An autonomous AI-powered research system that discovers, validates, synthesizes, and monitors academic research across multiple sources using a multi-agent architecture.

## ğŸŒŸ Features

- **ğŸ” Multi-Source Discovery**: Searches ArXiv, Semantic Scholar, and the web simultaneously
- **âœ… Smart Validation**: AI-powered credibility scoring and relevance checking
- **ğŸ§¬ Knowledge Synthesis**: Builds knowledge graphs, identifies consensus, gaps, and contradictions
- **ğŸ¤– ML Analysis**: Topic modeling (LDA), paper clustering (K-means), citation prediction
- **ğŸ“Š Comprehensive Reporting**: Generates detailed research reports with visualizations
- **ğŸ“¡ Trend Monitoring**: Analyzes emerging topics and tracks research trends
- **ğŸ¨ Interactive UI**: Beautiful Streamlit web interface for easy interaction
- **ğŸ”„ Production Ready**: Apache Airflow integration for workflow orchestration

## ğŸ—ï¸ Architecture

```
Multi-Agent System (7 Specialized Agents)
â”œâ”€â”€ Discovery Agent: Multi-source search
â”œâ”€â”€ Validation Agent: Quality scoring
â”œâ”€â”€ RAG Agent: Vector embeddings (ChromaDB)
â”œâ”€â”€ Synthesis Agent: Knowledge graph building
â”œâ”€â”€ ML Agent: Machine learning analysis
â”œâ”€â”€ Reporter Agent: Report generation
â””â”€â”€ Monitoring Agent: Trend analysis

Powered by: LangGraph + Groq LLM + ChromaDB
```

## ğŸ“ Project Structure

```
datascienceproj/
â”œâ”€â”€ 1_Documentation/          # Problem statements, docs
â”œâ”€â”€ 2_Data/                   # Raw and processed data
â”‚   â”œâ”€â”€ raw/                  # Raw research data
â”‚   â””â”€â”€ processed/            # Validated sources
â”œâ”€â”€ 3_Notebooks/              # Jupyter notebooks for testing
â”œâ”€â”€ 4_Src_Code/               # Main source code
â”‚   â”œâ”€â”€ agents/               # Individual agent modules
â”‚   â””â”€â”€ agentic_ai_pipeline.py  # Main pipeline
â”œâ”€â”€ 5_Pipeline/               # Airflow orchestration
â”‚   â””â”€â”€ airflow_dag.py        # Production DAG
â”œâ”€â”€ 6_Models/                 # ML models and vector stores
â”‚   â””â”€â”€ vectorstore/          # ChromaDB storage
â”œâ”€â”€ 7_Results/                # Generated outputs
â”‚   â”œâ”€â”€ knowledge_graph.png
â”‚   â””â”€â”€ research_reports/
â”œâ”€â”€ 8_Demo/                   # Demo materials
â”‚   â””â”€â”€ screenshots/
â”œâ”€â”€ 9_Deployment/             # Deployment files
â”‚   â””â”€â”€ app.py                # Streamlit UI
â”œâ”€â”€ requirements.txt          # Python dependencies
â””â”€â”€ README.md                 # This file
```

## ğŸš€ Quick Start

### 1. Installation

```bash
# Clone the repository
git clone <your-repo-url>
cd datascienceproj

# Install dependencies
pip install -r requirements.txt
```

### 2. Environment Setup

Create a `.env` file with your API keys:

**Step 1: Copy the example file**
```bash
# Windows
rename env.example .env

# Mac/Linux
mv env.example .env
```

**Step 2: Edit `.env` and add your keys**
```env
GROQ_API_KEY=your_groq_api_key_here
SERPAPI_KEY=your_serpapi_key_here  # Optional, for web search
```

Get your API keys:
- **Groq API Key** (Required): https://console.groq.com/
- **SERPAPI Key** (Optional): https://serpapi.com/

### 3. Run Options

#### Option A: Streamlit Web UI (Recommended)

```bash
streamlit run 9_Deployment/app.py
```

Then open your browser to `http://localhost:8501`

#### Option B: Command Line

```bash
python 4_Src_Code/agentic_ai_pipeline.py
```

#### Option C: Airflow Orchestration (Production)

```bash
# See 5_Pipeline/README.md for Airflow setup
docker-compose -f docker-compose-airflow.yml up -d
```

## ğŸ“– Usage Example

### Web UI
1. Open the Streamlit app
2. Enter your API keys in the sidebar
3. Type your research query (e.g., "Transformer models for computer vision")
4. Select research depth (quick/standard/deep)
5. Click "Start Research"
6. View results, download reports, and explore knowledge graphs

### Python API
```python
from agentic_ai_pipeline import run_research_pipeline

result = run_research_pipeline(
    query="Large Language Models for code generation",
    research_depth="standard"
)

print(result['executive_summary'])
print(f"Found {len(result['validated_sources'])} validated sources")
```

## ğŸ§  How It Works

1. **Discovery Phase**: Searches multiple sources (ArXiv, Semantic Scholar, Web)
2. **Validation Phase**: Scores each source for credibility and relevance
3. **RAG Integration**: Creates vector embeddings in ChromaDB for semantic search
4. **Synthesis Phase**: Builds knowledge graph, finds consensus and gaps
5. **ML Analysis**: Performs topic modeling, clustering, and predictions
6. **Reporting Phase**: Generates comprehensive research report
7. **Monitoring Phase**: Sets up alerts and trend tracking

## ğŸ“Š Output Files

After running a research query, you'll get:

- **Research Report** (`report_*.txt`): Comprehensive analysis
- **Knowledge Graph** (`kg_*.png`): Visual representation of concepts
- **Vector Database** (`chroma_db/`): Semantic search embeddings
- **JSON Data**: Structured research data for further processing

## ğŸ› ï¸ Technologies

- **LLM Framework**: LangChain, LangGraph
- **LLM Provider**: Groq (llama-3.1-8b-instant)
- **Vector DB**: ChromaDB
- **Embeddings**: HuggingFace sentence-transformers
- **ML**: scikit-learn (LDA, K-means)
- **Orchestration**: Apache Airflow
- **UI**: Streamlit
- **Visualization**: matplotlib, networkx

## ğŸ“š Documentation

- **Pipeline Documentation**: See `4_Src_Code/README.md`
- **Airflow Setup**: See `5_Pipeline/README.md`
- **Deployment Guide**: See `9_Deployment/README.md`

## ğŸ”§ Configuration

### Research Depth Options

- **Quick** (~5 min): 10-15 sources, basic analysis
- **Standard** (~8 min): 20-25 sources, full analysis
- **Deep** (~12 min): 30+ sources, comprehensive analysis

### Customization

You can customize agent behavior by modifying:
- `4_Src_Code/agentic_ai_pipeline.py`: Agent logic and prompts
- `5_Pipeline/airflow_dag.py`: Workflow orchestration
- `9_Deployment/app.py`: UI appearance and features

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## ğŸ“„ License

This project is licensed under the MIT License.

## ğŸ™ Acknowledgments

- **LangChain**: For the amazing agent framework
- **Groq**: For fast LLM inference
- **Streamlit**: For the beautiful UI framework
- **ArXiv & Semantic Scholar**: For open access to research

## ğŸ“ Support

For issues, questions, or feedback:
- Create an issue on GitHub
- Contact the maintainers

---

**Built with â¤ï¸ for the research community** | Â© 2025

